2023/03/24

긴 오디오 파일을 잘라서 추론하고 다시 붙이는 방법 연구 :
    포즈가 있는 부분을 찾아서 자르기
    
참고(오디오 청크) : https://github.com/tyiannak/pyAudioAnalysis

evaluation 프로세스

1. 오디오 파일, 라벨 가져오기
2. 오디오 파일 시간 체크
    30초 이하 -> 인식 프로세스
    30초 초과 -> 자르기
        -> 문장 별로 자르기
            -> 문장이 30초 이하 -> 인식 프로세스
            -> 문장이 30초 초과 -> 자르기
                -> 인식 프로세스
                -> 결과 합치기
        -> 결과 합치기
3. 인식 결과를 라벨과 띄어쓰기 교정
4. 비교: 에러레이트 계산



2023/03/23
사용법
1. 데이터 준비 : 오디오 파일을 문장별로 자르고 매니페스트 파일 만들기
    1. aidata_make_manifest.py # 뒷부분의 province_code를 원하는 지역으로 설정
    2. sentence_dir로 지정된 디렉토리에 문장별로 나누어진 음성 파일과 매니페스트 파일(csv형식) 저장

2. 토큰화 : 
    1. token_train.py
    2. 한 번 실행으로 모든 지역에 대하여 각각 토큰화 학습
    3. 결과는 ./results/<province_code> 생성

3. 학습(Fine Tuning)



실행 순서
1. aidata_make_manifest.py 
    -> province_code.csv 파일 생성 # 예) gw.csv

2. aidata_split_manifest.py
    province + "_train.csv # 예) gw_train.csv
    province + "_valid.csv # 예) gw_valid.csv
    province + "_test.csv # 예) gw_test.csv



평가(evaluation) 데이터 준비
시간 : 3 ~ 40 초
전체의 10% (시간)
    강원 : 80
    경상 : 120
    전라 : 100
    제주 : 20
    충청 : 80


sWER 처리 : 완료 2023/02/26

1. sWER 계산 : ASR train.py에서 swer 계산 추가
    - self.swer_metric = self.hparams.error_rate_computer()
    - predicted_swords = get_swords(wrd, hyps) -> space normalized words
        - 변환하는 함수 모듈로 만들어서 불러오기.
    - self.swer_metric.append(ids, predicted_swords, target_words)
    - self.swer_metric.write_stats(w)
        -> utils/metric_stats/MetricStats/write_stats

2. space normalize 함수 만들기
    - swer.ipynb 파일 내용을 참고하여 만들기
    -> get_swords()

3. utils/metric_stats.py ErrorRateStats write_stats
    -> metric_stats_sc.py를 만들어서 metric_stats 대신 사용
        from speechbrain.dataio.swer import print_wer_summary, print_swer_summary, print_cer_summary, print_alignments

    wer 대신 swer을 import

    316 행 부근에 아래 추가
    print_swer_summary
    print_cer_summary

    self.swer_metric.write_stats(w) -> self.swer_metric.write_stats(w, "swer")
    self.wer_metric.write_stats(w) -> self.swer_metric.write_stats(w, "wer")
    self.cer_metric.write_stats(w) -> self.swer_metric.write_stats(w, "cer")

    yaml에서 metric_stats 수정
    error_rate_computer: !name:speechbrain.utils.metric_stats_sc.ErrorRateStats

4. log : dataio/swer.py를 만들어서 wer.py 대신 사용
    - print_wer_summary(): # 30행, 다음에 print_swer_summary() print_cer_summary() 추가

5. evaluate.py에서 아래와 같이 metric type을 지정하여 실행
    self.swer_metric.write_stats(w, "swer") : 171행



데이터 검사 필요 : wer값이 높게 나온 데이터
41 : 90.42
68 : 82.52
40 : 85.52
69 : 83.52
6 : 77.51
20 : 76.66
70 : 77.97
153 : 75.91
174 : 75.01
117 : 74.97
119 : 75.03
65 : 74.66
