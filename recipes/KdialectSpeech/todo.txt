
평가(evaluation) 데이터 준비
시간 : 3 ~ 40 초
전체의 10% (시간)
    강원 : 80
    경상 : 120
    전라 : 100
    제주 : 20
    충청 : 80


sWER 처리 : 완료 2023/02/26

1. sWER 계산 : ASR train.py에서 swer 계산 추가
    - self.swer_metric = self.hparams.error_rate_computer()
    - predicted_swords = get_swords(wrd, hyps) -> space normalized words
        - 변환하는 함수 모듈로 만들어서 불러오기.
    - self.swer_metric.append(ids, predicted_swords, target_words)
    - self.swer_metric.write_stats(w)
        -> utils/metric_stats/MetricStats/write_stats

2. space normalize 함수 만들기
    - swer.ipynb 파일 내용을 참고하여 만들기
    -> get_swords()

3. utils/metric_stats.py ErrorRateStats write_stats
    -> metric_stats_sc.py를 만들어서 metric_stats 대신 사용
        from speechbrain.dataio.swer import print_wer_summary, print_swer_summary, print_cer_summary, print_alignments

    wer 대신 swer을 import

    316 행 부근에 아래 추가
    print_swer_summary
    print_cer_summary

    self.swer_metric.write_stats(w) -> self.swer_metric.write_stats(w, "swer")
    self.wer_metric.write_stats(w) -> self.swer_metric.write_stats(w, "wer")
    self.cer_metric.write_stats(w) -> self.swer_metric.write_stats(w, "cer")

    yaml에서 metric_stats 수정
    error_rate_computer: !name:speechbrain.utils.metric_stats_sc.ErrorRateStats

4. log : dataio/swer.py를 만들어서 wer.py 대신 사용
    - print_wer_summary(): # 30행, 다음에 print_swer_summary() print_cer_summary() 추가

5. evaluate.py에서 아래와 같이 metric type을 지정하여 실행
    self.swer_metric.write_stats(w, "swer") : 171행



데이터 검사 필요 : wer값이 높게 나온 데이터
41 : 90.42
68 : 82.52
40 : 85.52
69 : 83.52
6 : 77.51
20 : 76.66
70 : 77.97
153 : 75.91
174 : 75.01
117 : 74.97
119 : 75.03
65 : 74.66
