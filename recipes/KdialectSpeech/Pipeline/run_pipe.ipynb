{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실행 모듈\n",
    "\n",
    "데이터 다운로드 : 다운로드할 디렉토리(object) 목록을 run_pipe.yaml에 설정  \n",
    "데이터 검증  \n",
    "데이터 리샘플링  \n",
    "토크나이저 실행(방언별로 각각 실행)  \n",
    "- 데이터 준비  \n",
    "- 토큰화  \n",
    "\n",
    "언어모델 실행(방언별로 각각 실행)  \n",
    "음성인식 실행(방언별로 각각 실행)  \n",
    "추론 준비 : 추론에 필요한 모델 파일을 한 디렉토리에 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperpyyaml\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "import speechbrain as sb\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정값 yaml에서 읽어오기\n",
    "# log file 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
    "hparams_file = 'run_pipe.yaml'\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "log_config = hparams[\"log_config\"]\n",
    "log_file = hparams[\"log_file\"]\n",
    "\n",
    "logger_overrides = {\n",
    "    \"handlers\": {\"file_handler\": {\"filename\": log_file}}\n",
    "}\n",
    "\n",
    "# setup_logging(config_path=\"log-config.yaml\", overrides={}, default_level=logging.INFO)\n",
    "sb.utils.logger.setup_logging(log_config, logger_overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### download\n",
    "\n",
    "# yaml에서 설정값 읽어오기 : 스토리지 접속 정보, 데이터 저장 위치\n",
    "\n",
    "service_name = hparams[\"service_name\"]\n",
    "endpoint_url = hparams[\"endpoint_url\"]\n",
    "region_name = hparams[\"region_name\"]\n",
    "access_key = hparams[\"access_key\"]\n",
    "secret_key = hparams[\"secret_key\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(service_name, endpoint_url=endpoint_url, aws_access_key_id=access_key,\n",
    "                    aws_secret_access_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_object_list(s3, bucket_name, prefix, max_keys):\n",
    "    obj_list = []\n",
    "    response = s3.list_objects(Bucket=bucket_name, MaxKeys=max_keys, Prefix=prefix)\n",
    "\n",
    "    while True:\n",
    "        if response.get('Contents') is not None:\n",
    "\n",
    "            for content in response.get('Contents'):\n",
    "                filename = content.get('Key')\n",
    "                date_info = content.get('LastModified')\n",
    "                \n",
    "                obj_list.append(filename)\n",
    "        \n",
    "            if response.get('IsTruncated'):\n",
    "                response = s3.list_objects(Bucket=bucket_name, MaxKeys=max_keys, Prefix=prefix,\n",
    "                                        Marker=response.get('NextMarker'))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            print(f'{prefix} : there is no data.')\n",
    "            break\n",
    "    \n",
    "    return obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_path = secret_key = hparams[\"data_save_path\"]\n",
    "os.makedirs(data_save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "bucket_name = hparams[\"bucket_name\"]\n",
    "max_keys = hparams[\"max_keys\"]\n",
    "key_names = hparams[\"key_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1214Dataset', '1215Dataset']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__ - 1214Dataset\n",
      "__main__ - ----- date download start -----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7542/7542 [11:58<00:00, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__ - ----- date download end -----\n",
      "\n",
      "__main__ - 1215Dataset\n",
      "__main__ - ----- date download start -----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 8210/8210 [13:53<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__ - ----- date download end -----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "error_file = 'error_' + datetime.datetime.now().strftime('%Y%m%d%H%M%S') + '.txt'\n",
    "error_file_path = os.path.join(data_save_path, error_file)\n",
    "with open(error_file_path, 'w') as lf:\n",
    "\n",
    "    for date in key_names:\n",
    "        logger.info(date)\n",
    "        logger.info(f'----- date download start -----\\n')\n",
    "        # object_list = get_s3_object_list(s3, bucket_name, date, max_keys)\n",
    "        # print(object_list[0])\n",
    "\n",
    "        # for key in tqdm(object_list):\n",
    "        for key in tqdm(get_s3_object_list(s3, bucket_name, date, max_keys)):\n",
    "            # print(Path(key).suffix)\n",
    "            if Path(key).suffix in ['.json', '.wav']:\n",
    "                key_2 = key.split('/')\n",
    "                save_dir = os.path.join(data_save_path, key_2[1], key_2[2], key_2[3], key_2[4])\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_file = os.path.join(save_dir, key_2[5])\n",
    "                try:\n",
    "                    # print(i)\n",
    "                    s3.download_file(bucket_name, key, save_file)\n",
    "                except:\n",
    "                    lf.write(f'{key}\\n')\n",
    "                    print(f'{key} is not exist.')\n",
    "                    continue\n",
    "            else:\n",
    "                lf.write(f'{key}\\n')\n",
    "                print(f'{key} is not file')\n",
    "\n",
    "        logger.info(f'----- date download end -----\\n')\n",
    "\n",
    "    lf.write('----- end time : ' + datetime.datetime.now().strftime('%Y%m%d%H%M%S'))\n",
    "lf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_dir = '/workspace/speechbrain/recipes/KdialectSpeech/ASR/Conformer'\n",
    "run_province = 'cc'\n",
    "train_result_dir = os.path.join(asr_dir, 'results/Conformer/5555/' + run_province + '/save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = sorted(glob.glob(train_result_dir + '/CKPT*'), key=os.path.getmtime)[0]\n",
    "best_model_dir = Path(best_model_dir).stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CKPT+2022-12-28+14-16-55+00'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = os.path.join(train_result_dir, best_model_dir, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/speechbrain/recipes/KdialectSpeech/ASR/Conformer/results/Conformer/5555/cc/save/CKPT+2022-12-28+14-16-55+00/model.ckpt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hparam_file = '../ASR/Conformer/Inference/pretrained-model-src/hyperparams.yaml'\n",
    "pretrained_model_dir = '../ASR/Conformer/Inference/pretrained-model-src/cc'\n",
    "os.makedirs(pretrained_model_dir, exist_ok=True)\n",
    "# best_model = 'CKPT+2022-12-28+14-16-55+00/model.ckpt'\n",
    "best_model_target = '../Inference/pretrained-model-src/cc/asr.ckpt'\n",
    "# hparam_file = '../Inference/pretrained-model-src/hyperparams.yaml'\n",
    "normalizer = '../Inference/pretrained-model-src/cc'\n",
    "\n",
    "hparam_file_target = os.path.join(pretrained_model_dir, 'hyperparams.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../ASR/Conformer/Inference/pretrained-model-src/cc'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../ASR/Conformer/Inference/pretrained-model-src/hyperparams.yaml'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparam_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../ASR/Conformer/Inference/pretrained-model-src/cc/hyperparams.yaml'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(hparam_file, hparam_file_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 3658 Dec 20 20:30 ../ASR/Conformer/Inference/pretrained-model-src/hyperparams.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls -al ../ASR/Conformer/Inference/pretrained-model-src/hyperparams.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/speechbrain/recipes/KdialectSpeech/Pipeline\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(hparam_file, pretrained_model_dir)\n",
    "shutil.copy(best_model, best_model_target)\n",
    "shutil.copy(normalizer, pretrained_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
