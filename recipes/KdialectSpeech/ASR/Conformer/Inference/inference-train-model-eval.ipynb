{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import speechbrain as sb\n",
    "from torch.utils.data import DataLoader\n",
    "from hyperpyyaml import load_hyperpyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = 'hparams/5k_conformer_medium_infer_no_lm.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASR(sb.core.Brain):\n",
    "    def compute_forward(self, batch):\n",
    "        \"\"\"Forward computations from the waveform batches\n",
    "        to the output probabilities.\"\"\"\n",
    "        \n",
    "        batch = batch.to(self.device)\n",
    "        wavs = batch\n",
    "        wav_lens = torch.tensor(1., device='cuda')\n",
    "\n",
    "        # compute features\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "        print(f'feats size ---- : {feats.size()}')\n",
    "        print(f'feats ---- : {feats}')\n",
    "        current_epoch = self.hparams.epoch_counter.current\n",
    "        print(f'current_epoch ----- : {current_epoch}')\n",
    "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
    "        print(f'feats ----- : {feats}')\n",
    "\n",
    "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
    "        \"\"\"perform checkpoint averge if needed\"\"\"\n",
    "        super().on_evaluate_start()\n",
    "\n",
    "        print(f'self.checkpointer checkpoints_dir ----- : {self.checkpointer.checkpoints_dir}')\n",
    "        # print(f'self.checkpointer ----- : {dir(self.checkpointer.checkpoints_dir)}')\n",
    "        ckpts = self.checkpointer.find_checkpoints(\n",
    "            max_key=max_key, min_key=min_key\n",
    "        )\n",
    "        print(f'ckpts ----- : {ckpts}')\n",
    "        # print(f'self.hparams.model.load_state_dict ----- : {self.hparams.model.load_state_dict}')\n",
    "        ckpt = sb.utils.checkpoints.average_checkpoints(\n",
    "            ckpts, recoverable_name=\"model\", device=self.device\n",
    "        )\n",
    "\n",
    "        self.hparams.model.load_state_dict(ckpt, strict=True)\n",
    "        self.hparams.model.eval()\n",
    "\n",
    "    ### for inferrence\n",
    "    def transcribe_file(\n",
    "            self,\n",
    "            data_file,\n",
    "            max_key, # We load the model with the lowest WER\n",
    "        ):\n",
    "        \n",
    "        sig = sb.dataio.dataio.read_audio(data_file)\n",
    "        print(f'sig ----- : {sig}')\n",
    "\n",
    "        self.on_evaluate_start(max_key=max_key) # We call the on_evaluate_start that will load the best model\n",
    "        # self.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "\n",
    "        # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "        with torch.no_grad():\n",
    "\n",
    "            transcripts = []\n",
    "            # for batch in tqdm(testdata, dynamic_ncols=True):\n",
    "            batch = sig.unsqueeze(dim=0)\n",
    "            out = self.compute_forward(batch)\n",
    "            predicted_tokens = out\n",
    "\n",
    "                # We go from tokens to words.\n",
    "            tokenizer = hparams[\"tokenizer\"]\n",
    "            predicted_words = [\n",
    "                tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in predicted_tokens\n",
    "            ]\n",
    "                \n",
    "            print(f'label : {batch.wrd}')\n",
    "            print(f'hyp ----- : {predicted_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"Adam\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "# adding objects to trainer:\n",
    "# asr_brain.tokenizer = hparams[\"tokenizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig ----- : tensor([-0.0010, -0.0016, -0.0012,  ...,  0.0003,  0.0003,  0.0007])\n",
      "self.checkpointer checkpoints_dir ----- : /data/models/0513/ascending/save\n",
      "ckpts ----- : []\n",
      "self.hparams.model.load_state_dict ----- : <bound method Module.load_state_dict of ModuleList(\n",
      "  (0): InputNormalization()\n",
      "  (1): ConvolutionFrontEnd(\n",
      "    (convblock_0): ConvBlock(\n",
      "      (convs): Sequential(\n",
      "        (conv_0): Conv2d(\n",
      "          (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "        (norm_0): LayerNorm(\n",
      "          (norm): LayerNorm((40, 64), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (act_0): LeakyReLU(negative_slope=0.01)\n",
      "        (dropout_0): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (convblock_1): ConvBlock(\n",
      "      (convs): Sequential(\n",
      "        (conv_0): Conv2d(\n",
      "          (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "        (norm_0): LayerNorm(\n",
      "          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (act_0): LeakyReLU(negative_slope=0.01)\n",
      "        (dropout_0): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): TransformerASR(\n",
      "    (positional_encoding): RelPosEncXL()\n",
      "    (positional_encoding_decoder): PositionalEncoding()\n",
      "    (encoder): ConformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (2): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (3): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (4): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (5): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (6): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (7): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (8): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (9): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (10): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (11): ConformerEncoderLayer(\n",
      "          (mha_layer): RelPosMHAXL(\n",
      "            (dropout_att): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (convolution_module): ConvolutionModule(\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (bottleneck): Sequential(\n",
      "              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              (1): GLU(dim=1)\n",
      "            )\n",
      "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "            (after_conv): Sequential(\n",
      "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): Swish(\n",
      "                (sigmoid): Sigmoid()\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ffn_module1): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_module2): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): PositionalwiseFeedForward(\n",
      "              (ffn): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (1): Swish(\n",
      "                  (sigmoid): Sigmoid()\n",
      "                )\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (mutihead_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (pos_ffn): PositionalwiseFeedForward(\n",
      "            (ffn): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm3): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.0, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (mutihead_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (pos_ffn): PositionalwiseFeedForward(\n",
      "            (ffn): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm3): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.0, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (2): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (mutihead_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (pos_ffn): PositionalwiseFeedForward(\n",
      "            (ffn): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm3): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.0, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (3): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (mutihead_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (pos_ffn): PositionalwiseFeedForward(\n",
      "            (ffn): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm3): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.0, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (4): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (mutihead_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (pos_ffn): PositionalwiseFeedForward(\n",
      "            (ffn): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm3): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.0, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (5): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (mutihead_attn): MultiheadAttention(\n",
      "            (att): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (pos_ffn): PositionalwiseFeedForward(\n",
      "            (ffn): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm2): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (norm3): LayerNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.0, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (custom_src_module): ModuleList(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(\n",
      "          (w): Linear(in_features=640, out_features=256, bias=True)\n",
      "        )\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (custom_tgt_module): ModuleList(\n",
      "      (layers): ModuleList(\n",
      "        (0): NormalizedEmbedding(\n",
      "          (emb): Embedding(\n",
      "            (Embedding): Embedding(5000, 256)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): Linear(\n",
      "    (w): Linear(in_features=256, out_features=5000, bias=True)\n",
      "  )\n",
      "  (4): Linear(\n",
      "    (w): Linear(in_features=256, out_features=5000, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No state dicts to average.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/workspace/speechbrain/speechbrain/utils/checkpoints.py\u001b[0m in \u001b[0;36maverage_state_dicts\u001b[0;34m(state_dicts)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0mrunning_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_820475/825960446.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/data/KsponSpeech/test/eval_clean/KsponSpeech_E02998.wav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m asr_brain.transcribe_file(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Must be obtained from the dataio_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ACC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# We load the model with the lowest WER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_820475/1104379035.py\u001b[0m in \u001b[0;36mtranscribe_file\u001b[0;34m(self, data_file, max_key)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'sig ----- : {sig}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_evaluate_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_key\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We call the on_evaluate_start that will load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m# self.modules.eval() # We set the model to eval mode (remove dropout etc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_820475/1104379035.py\u001b[0m in \u001b[0;36mon_evaluate_start\u001b[0;34m(self, max_key, min_key)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'ckpts ----- : {ckpts}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'self.hparams.model.load_state_dict ----- : {self.hparams.model.load_state_dict}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         ckpt = sb.utils.checkpoints.average_checkpoints(\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mckpts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecoverable_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         )\n",
      "\u001b[0;32m/workspace/speechbrain/speechbrain/utils/checkpoints.py\u001b[0m in \u001b[0;36maverage_checkpoints\u001b[0;34m(checkpoint_list, recoverable_name, parameter_loader, averager, device)\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         )\n\u001b[0;32m-> 1181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maverager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/speechbrain/speechbrain/utils/checkpoints.py\u001b[0m in \u001b[0;36maverage_state_dicts\u001b[0;34m(state_dicts)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0mrunning_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No state dicts to average.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m     \u001b[0mnum_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No state dicts to average."
     ]
    }
   ],
   "source": [
    "audio_file = '/data/KsponSpeech/test/eval_clean/KsponSpeech_E02998.wav'\n",
    "\n",
    "asr_brain.transcribe_file(\n",
    "    audio_file, # Must be obtained from the dataio_function\n",
    "    max_key=\"ACC\", # We load the model with the lowest WER\n",
    "    # loader_kwargs=hparams[\"test_dataloader_opts\"], # opts for the dataloading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:59:51) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
