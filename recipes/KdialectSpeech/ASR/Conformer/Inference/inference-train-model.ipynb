{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import speechbrain as sb\n",
    "from torch.utils.data import DataLoader\n",
    "from hyperpyyaml import load_hyperpyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = 'hparams/5k_conformer_medium_infer_no_lm.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASR(sb.core.Brain):\n",
    "    def compute_forward(self, batch):\n",
    "        \"\"\"Forward computations from the waveform batches\n",
    "        to the output probabilities.\"\"\"\n",
    "        \n",
    "        batch = batch.to(self.device)\n",
    "        wavs = batch\n",
    "        wav_lens = torch.tensor(1., device='cuda')\n",
    "        # wavs, wav_lens = batch.sig\n",
    "        # tokens_bos, _ = batch.tokens_bos\n",
    "        print(f'wav_lens ----- : {wav_lens}')\n",
    "\n",
    "        # compute features\n",
    "        print(f'wavs ---- : {wavs}')\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "        print(f'feats size ---- : {feats.size()}')\n",
    "        print(f'feats ---- : {feats}')\n",
    "        current_epoch = self.hparams.epoch_counter.current\n",
    "        print(f'current_epoch ----- : {current_epoch}')\n",
    "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
    "\n",
    "        print(f'feats ----- : {feats}')\n",
    "\n",
    "        # forward modules\n",
    "        # src = self.modules.CNN(feats)\n",
    "        # enc_out, pred = self.modules.Transformer( # pred : decoder out\n",
    "        #     src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index\n",
    "        # )\n",
    "\n",
    "        # hyps = None\n",
    "        # hyps, _ = self.hparams.valid_search(enc_out.detach(), wav_lens) # Valid\n",
    "        # hyps, _ = self.hparams.test_search(enc_out.detach(), wav_lens) # Test\n",
    "        # return hyps\n",
    "\n",
    "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
    "        \"\"\"perform checkpoint averge if needed\"\"\"\n",
    "        super().on_evaluate_start()\n",
    "\n",
    "        print(f'self.checkpointer checkpoints_dir ----- : {self.checkpointer.checkpoints_dir}')\n",
    "        # print(f'self.checkpointer ----- : {dir(self.checkpointer.checkpoints_dir)}')\n",
    "        ckpts = self.checkpointer.find_checkpoints(\n",
    "            max_key=max_key, min_key=min_key\n",
    "        )\n",
    "        print(f'ckpts ----- : {ckpts}')\n",
    "        ckpt = sb.utils.checkpoints.average_checkpoints(\n",
    "            ckpts, recoverable_name=\"model\", device=self.device\n",
    "        )\n",
    "\n",
    "        self.hparams.model.load_state_dict(ckpt, strict=True)\n",
    "        self.hparams.model.eval()\n",
    "\n",
    "\n",
    "    ### for inferrence\n",
    "    def transcribe_file(\n",
    "            self,\n",
    "            data_file,\n",
    "            max_key, # We load the model with the lowest WER\n",
    "        ):\n",
    "        \n",
    "        sig = sb.dataio.dataio.read_audio(data_file)\n",
    "        print(f'sig ----- : {sig}')\n",
    "\n",
    "        self.on_evaluate_start(max_key=max_key) # We call the on_evaluate_start that will load the best model\n",
    "        # self.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "\n",
    "        # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "        with torch.no_grad():\n",
    "\n",
    "            transcripts = []\n",
    "            # for batch in tqdm(testdata, dynamic_ncols=True):\n",
    "            batch = sig.unsqueeze(dim=0)\n",
    "            out = self.compute_forward(batch)\n",
    "            predicted_tokens = out\n",
    "\n",
    "                # We go from tokens to words.\n",
    "            tokenizer = hparams[\"tokenizer\"]\n",
    "            predicted_words = [\n",
    "                tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in predicted_tokens\n",
    "            ]\n",
    "                \n",
    "            print(f'label : {batch.wrd}')\n",
    "            print(f'hyp ----- : {predicted_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"Adam\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "# adding objects to trainer:\n",
    "# asr_brain.tokenizer = hparams[\"tokenizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = '/data/KsponSpeech/eval_clean_wav/KsponSpeech_E02998.wav'\n",
    "\n",
    "asr_brain.transcribe_file(\n",
    "    audio_file, # Must be obtained from the dataio_function\n",
    "    max_key=\"ACC\", # We load the model with the lowest WER\n",
    "    # loader_kwargs=hparams[\"test_dataloader_opts\"], # opts for the dataloading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ckpt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
