{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import speechbrain as sb\n",
    "from torch.utils.data import DataLoader\n",
    "from hyperpyyaml import load_hyperpyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = 'hparams/5k_conformer_medium_infer_no_lm.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASR(sb.core.Brain):\n",
    "    def compute_forward(self, batch):\n",
    "        \"\"\"Forward computations from the waveform batches\n",
    "        to the output probabilities.\"\"\"\n",
    "        \n",
    "        batch = batch.to(self.device)\n",
    "        wavs = batch\n",
    "        wav_lens = torch.tensor(1., device='cuda')\n",
    "        # wavs, wav_lens = batch.sig\n",
    "        # tokens_bos, _ = batch.tokens_bos\n",
    "        print(f'wav_lens ----- : {wav_lens}')\n",
    "\n",
    "        # compute features\n",
    "        print(f'wavs ---- : {wavs}')\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "        print(f'feats size ---- : {feats.size()}')\n",
    "        print(f'feats ---- : {feats}')\n",
    "        current_epoch = self.hparams.epoch_counter.current\n",
    "        print(f'current_epoch ----- : {current_epoch}')\n",
    "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
    "\n",
    "        print(f'feats ----- : {feats}')\n",
    "\n",
    "        # forward modules\n",
    "        # src = self.modules.CNN(feats)\n",
    "        # enc_out, pred = self.modules.Transformer( # pred : decoder out\n",
    "        #     src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index\n",
    "        # )\n",
    "\n",
    "        # hyps = None\n",
    "        # hyps, _ = self.hparams.valid_search(enc_out.detach(), wav_lens) # Valid\n",
    "        # hyps, _ = self.hparams.test_search(enc_out.detach(), wav_lens) # Test\n",
    "        # return hyps\n",
    "\n",
    "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
    "        \"\"\"perform checkpoint averge if needed\"\"\"\n",
    "        super().on_evaluate_start()\n",
    "\n",
    "        print(f'self.checkpointer checkpoints_dir ----- : {self.checkpointer.checkpoints_dir}')\n",
    "        # print(f'self.checkpointer ----- : {dir(self.checkpointer.checkpoints_dir)}')\n",
    "        ckpts = self.checkpointer.find_checkpoints(\n",
    "            max_key=max_key, min_key=min_key\n",
    "        )\n",
    "        print(f'ckpts ----- : {ckpts}')\n",
    "        ckpt = sb.utils.checkpoints.average_checkpoints(\n",
    "            ckpts, recoverable_name=\"model\", device=self.device\n",
    "        )\n",
    "\n",
    "        self.hparams.model.load_state_dict(ckpt, strict=True)\n",
    "        self.hparams.model.eval()\n",
    "\n",
    "\n",
    "    ### for inferrence\n",
    "    def transcribe_file(\n",
    "            self,\n",
    "            data_file,\n",
    "            max_key, # We load the model with the lowest WER\n",
    "        ):\n",
    "        \n",
    "        sig = sb.dataio.dataio.read_audio(data_file)\n",
    "        print(f'sig ----- : {sig}')\n",
    "\n",
    "        self.on_evaluate_start(max_key=max_key) # We call the on_evaluate_start that will load the best model\n",
    "        # self.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "\n",
    "        # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "        with torch.no_grad():\n",
    "\n",
    "            transcripts = []\n",
    "            # for batch in tqdm(testdata, dynamic_ncols=True):\n",
    "            batch = sig.unsqueeze(dim=0)\n",
    "            out = self.compute_forward(batch)\n",
    "            predicted_tokens = out\n",
    "\n",
    "                # We go from tokens to words.\n",
    "            tokenizer = hparams[\"tokenizer\"]\n",
    "            predicted_words = [\n",
    "                tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in predicted_tokens\n",
    "            ]\n",
    "                \n",
    "            print(f'label : {batch.wrd}')\n",
    "            print(f'hyp ----- : {predicted_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"Adam\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "# adding objects to trainer:\n",
    "# asr_brain.tokenizer = hparams[\"tokenizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load audio from /data/KsponSpeech/eval_clean_wav/KsponSpeech_E02998.wav",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3136478/2720624502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/data/KsponSpeech/eval_clean_wav/KsponSpeech_E02998.wav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m asr_brain.transcribe_file(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Must be obtained from the dataio_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ACC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# We load the model with the lowest WER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3136478/4141763852.py\u001b[0m in \u001b[0;36mtranscribe_file\u001b[0;34m(self, data_file, max_key)\u001b[0m\n\u001b[1;32m     58\u001b[0m         ):\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'sig ----- : {sig}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/speechbrain/speechbrain/dataio/dataio.py\u001b[0m in \u001b[0;36mread_audio\u001b[0;34m(waveforms_obj)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \"\"\"\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveforms_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveforms_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaveforms_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_fallback_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36m_fail_load\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to load audio from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load audio from /data/KsponSpeech/eval_clean_wav/KsponSpeech_E02998.wav"
     ]
    }
   ],
   "source": [
    "audio_file = '/data/KsponSpeech/eval_clean_wav/KsponSpeech_E02998.wav'\n",
    "\n",
    "asr_brain.transcribe_file(\n",
    "    audio_file, # Must be obtained from the dataio_function\n",
    "    max_key=\"ACC\", # We load the model with the lowest WER\n",
    "    # loader_kwargs=hparams[\"test_dataloader_opts\"], # opts for the dataloading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
