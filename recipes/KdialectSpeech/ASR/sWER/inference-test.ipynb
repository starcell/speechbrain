{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본은 내 pc(Dell)의 deepcell-speechbrain-2 Docker에 있음, file name : inference.ipynb\n",
    "# KsponSpeech data로 테스트\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from speechbrain.pretrained import EncoderDecoderASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu로 추론, GPU가 있으면 GPU로 해야 빠름\n",
    "\n",
    "asr_model = EncoderDecoderASR.from_hparams(\n",
    "    source=\"ddwkim/asr-conformer-transformerlm-ksponspeech\", \n",
    "    savedir=\"pretrained_models/asr-conformer-transformerlm-ksponspeech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_0 = \"/data/ksponmini/test/eval_clean/KsponSpeech_E00099.wav\"\n",
    "trs = asr_model.transcribe_file(audio_0)\n",
    "trs\n",
    "\n",
    "# -> '반말을 할 수가 없어'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = '/data/ksponmini/test/eval_clean/*'\n",
    "\n",
    "file_list = glob.glob(audio_dir) # audio file path\n",
    "file_list_wav = [file for file in file_list if file.endswith(\".wav\")] # wav 파일만 모아서 리스트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오디오 파일을 텍스트 파일로 변화\n",
    "\n",
    "trans = []\n",
    "for file in tqdm(file_list_wav):\n",
    "    # print(os.path.basename(file))\n",
    "    # print(file)\n",
    "    file_name = Path(file).stem # 확장자 제거\n",
    "    id = file_name.split('_')[1] # _ 앞부분 제거하여 ID만 추출\n",
    "    trs = asr_model.transcribe_file(file)   # 음성인식\n",
    "    trans.append([id, trs])\n",
    "    # print(id)\n",
    "\n",
    "trans[:2] # 앞의 2개로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트롤 csv 파일로 저장\n",
    "hyp_file = 'hyp.csv'\n",
    "with open(hyp_file, 'w') as f:\n",
    "    for item in trans:\n",
    "        # write each item on a new line\n",
    "        f.write(f\"{item[0]},{item[1]}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_df = pd.DataFrame(trans, columns = ['spk_id', 'hyp'])\n",
    "# hyp_df = pd.read_csv(hyp_file, names=['spk_id', 'hyp'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '/speechbrain/recipes/KsponMini/Tokenizer/results/5K_subword_unigram_LM/eval_clean.csv'\n",
    "test_df = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyp_df와 test_df를 합치기\n",
    "merged_df = pd.merge(hyp_df, test_df, how='inner', on='spk_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_label_df = merged_df[['spk_id', 'hyp', 'wrd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_label_df.to_csv('hyp_label.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:59:51) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
